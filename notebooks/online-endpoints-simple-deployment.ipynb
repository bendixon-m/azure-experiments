{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy and score a machine learning model by using an online endpoint \n",
    "\n",
    "Learn how to use an online endpoint to deploy your model, so you don't have to create and manage the underlying infrastructure. You'll begin by deploying a model on your local machine to debug any errors, and then you'll deploy and test it in Azure.\n",
    "\n",
    "Managed online endpoints help to deploy your ML models in a turnkey manner. Managed online endpoints work with powerful CPU and GPU machines in Azure in a scalable, fully managed way. Managed online endpoints take care of serving, scaling, securing, and monitoring your models, freeing you from the overhead of setting up and managing the underlying infrastructure. \n",
    "\n",
    "For more information, see [What are Azure Machine Learning endpoints?](https://learn.microsoft.com/azure/machine-learning/concept-endpoints), and [Deploy an ML model with an online endpoint](https://learn.microsoft.com/azure/machine-learning/how-to-deploy-online-endpoints)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* To use Azure Machine Learning, you must have an Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n",
    "\n",
    "* Install and configure the [Python SDK v2](sdk/setup.sh).\n",
    "\n",
    "* You must have an Azure resource group, and you (or the service principal you use) must have Contributor access to it.\n",
    "\n",
    "* You must have an Azure Machine Learning workspace. \n",
    "\n",
    "* To deploy locally, you must install Docker Engine on your local computer. We highly recommend this option, so it's easier to debug issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mA web browser has been opened at https://login.microsoftonline.com/organizations/oauth2/v2.0/authorize. Please continue the login in the web browser. If no web browser is available or if the web browser fails to open, use device code flow with `az login --use-device-code`.\u001b[0m\n",
      "[\n",
      "  {\n",
      "    \"cloudName\": \"AzureCloud\",\n",
      "    \"homeTenantId\": \"7b658efc-8cee-4580-b989-85db158d4e3c\",\n",
      "    \"id\": \"3979595d-b401-4ff4-b3ef-661b8481c742\",\n",
      "    \"isDefault\": true,\n",
      "    \"managedByTenants\": [\n",
      "      {\n",
      "        \"tenantId\": \"961897a7-5808-4b80-b503-dbb7be13e7a7\"\n",
      "      }\n",
      "    ],\n",
      "    \"name\": \"MV-AZU-001\",\n",
      "    \"state\": \"Enabled\",\n",
      "    \"tenantId\": \"7b658efc-8cee-4580-b989-85db158d4e3c\",\n",
      "    \"user\": {\n",
      "      \"name\": \"ben.dixon@multiverse.io\",\n",
      "      \"type\": \"user\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "!az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben.dixon/Repos/azure-experiments/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    "    CodeConfiguration,\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Configure workspace details and get a handle to the workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /Users/ben.dixon/Repos/azure-experiments/config.json\n"
     ]
    }
   ],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient.from_config(credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and debug locally by using local endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "* To deploy locally, [Docker Engine](https://docs.docker.com/engine/install/) must be installed.\n",
    "* Docker Engine must be running. Docker Engine typically starts when the computer starts. If it doesn't, you can [troubleshoot Docker Engine](https://docs.docker.com/config/daemon/#start-the-daemon-manually)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define endpoint and deployment\n",
    "\n",
    "## 2.1 Define the endpoint\n",
    "\n",
    "To define an endpoint, you need to specify:\n",
    "\n",
    "* Endpoint name: The name of the endpoint. It must be unique in the Azure region. For more information on the naming rules, see [managed online endpoint limits](how-to-manage-quotas.md#azure-machine-learning-managed-online-endpoints).\n",
    "* Authentication mode: The authentication method for the endpoint. Choose between key-based authentication and Azure Machine Learning token-based authentication. A key doesn't expire, but a token does expire. For more information on authenticating, see [Authenticate to an online endpoint](how-to-authenticate-online-endpoint.md).\n",
    "* Optionally, you can add a description and tags to your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an endpoint name\n",
    "endpoint_name = \"my-endpoint\"\n",
    "\n",
    "# Example way to define a random name\n",
    "import datetime\n",
    "\n",
    "endpoint_name = \"endpt-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=endpoint_name,\n",
    "    description=\"this is a sample online endpoint\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\"foo\": \"bar\"},\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Define the deployment\n",
    "\n",
    "A deployment is a set of resources required for hosting the model that does the actual inferencing. To deploy a model, you must have:\n",
    "\n",
    "- Model files (or the name and version of a model that's already registered in your workspace). In the example, we have a scikit-learn model that does regression.\n",
    "- A scoring script, that is, code that executes the model on a given input request. The scoring script receives data submitted to a deployed web service and passes it to the model. The script then executes the model and returns its response to the client. The scoring script is specific to your model and must understand the data that the model expects as input and returns as output. In this example, we have a *score.py* file.\n",
    "- An environment in which your model runs. The environment can be a Docker image with Conda dependencies or a Dockerfile.\n",
    "- Settings to specify the instance type and scaling capacity.\n",
    "\n",
    "The following table describes the key attributes of a deployment:\n",
    "\n",
    "| Attribute      | Description                                                                                                                                                                                                                                                                                                                                                                                    |\n",
    "|-----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Name           | The name of the deployment.                                                                                                                                                                                                                                                                                                                                                                    |\n",
    "| Endpoint name  | The name of the endpoint to create the deployment under.                                                                                                                                                                                                                                                                                                                                       |\n",
    "| Model          | The model to use for the deployment. This value can be either a reference to an existing versioned model in the workspace or an inline model specification.                                                                                                                                                                                                                                    |\n",
    "| Code path      | The path to the directory on the local development environment that contains all the Python source code for scoring the model. You can use nested directories and packages.                                                                                                                                                                                                                    |\n",
    "| Scoring script | The relative path to the scoring file in the source code directory. This Python code must have an `init()` function and a `run()` function. The `init()` function will be called after the model is created or updated (you can use it to cache the model in memory, for example). The `run()` function is called at every invocation of the endpoint to do the actual scoring and prediction. |\n",
    "| Environment    | The environment to host the model and code. This value can be either a reference to an existing versioned environment in the workspace or an inline environment specification.                                                                                                                                                                                                                 |\n",
    "| Instance type  | The VM size to use for the deployment. For the list of supported sizes, see [Managed online endpoints SKU list](reference-managed-online-endpoints-vm-sku-list.md).                                                                                                                                                                                                                            |\n",
    "| Instance count | The number of instances to use for the deployment. Base the value on the workload you expect. For high availability, we recommend that you set the value to at least `3`. We reserve an extra 20% for performing upgrades. For more information, see [managed online endpoint quotas](how-to-manage-quotas.md#azure-machine-learning-managed-online-endpoints).                                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(path=\"model-1/model/sklearn_regression_model.pkl\")\n",
    "env = Environment(\n",
    "    conda_file=\"model-1/environment/conda.yaml\",\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    ")\n",
    "\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=model,\n",
    "    environment=env,\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"model-1/onlinescoring\", scoring_script=\"score.py\"\n",
    "    ),\n",
    "    instance_type=\"Standard_DS3_v2\",\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create local endpoint and deployment\n",
    "\n",
    "## 3.1 Create local endpoint\n",
    "\n",
    "The goal of a local endpoint deployment is to validate and debug your code and configuration before you deploy to Azure. Local deployment has the following limitations:\n",
    "* Local endpoints *do not support* traffic rules, authentication, or probe settings.\n",
    "* Local endpoints support only one deployment per endpoint.\n",
    "* They support local model files only. If you want to test registered models, first download them, then use `path` in the deployment definition to refer to the parent folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating local endpoint (endpt-05240930390654) .Done (0m 5s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ManagedOnlineEndpoint({'public_network_access': None, 'provisioning_state': None, 'scoring_uri': None, 'openapi_uri': None, 'name': 'endpt-05240930390654', 'description': 'this is a sample online endpoint', 'tags': {'foo': 'bar'}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': PosixPath('/Users/ben.dixon/.azureml/inferencing/endpt-05240930390654'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x118f2d610>, 'auth_mode': 'key', 'location': None, 'identity': None, 'traffic': {}, 'mirror_traffic': {}, 'kind': None})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_endpoints.begin_create_or_update(endpoint, local=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Create local deployment\n",
    "\n",
    "Now, create a deployment named `blue` under the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating local deployment (endpt-05240930390654 / blue) .\n",
      "Building Docker image from Dockerfile\n",
      "Step 1/6 : FROM mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\n",
      "... ---> 948ed3518cc2\n",
      "Step 2/6 : RUN mkdir -p /var/azureml-app/\n",
      " ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n",
      " ---> Running in 8cedf404cce1\n",
      " ---> 1bdcce09aeba\n",
      "Step 3/6 : WORKDIR /var/azureml-app/\n",
      " ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n",
      " ---> Running in 751a364096ea\n",
      " ---> 950e75f06919\n",
      "Step 4/6 : COPY conda.yml /var/azureml-app/\n",
      " ---> 5490974d81d2\n",
      "Step 5/6 : RUN conda env create -n inf-conda-env --file conda.yml\n",
      " ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n",
      " ---> Running in 658365c0e5bb\n",
      "Retrieving notices: ...working... done\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): ...working... ...done\n",
      "Solving environment: ...working... done\n",
      "\u001b[91m\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 24.4.0\n",
      "    latest version: 24.5.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\u001b[0m.\n",
      "Downloading and Extracting Packages: ...working... done\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... .done\n",
      "Executing transaction: ...working... done\n",
      "Installing pip dependencies: ...working... ..........Ran pip subprocess with arguments:\n",
      "['/opt/miniconda/envs/inf-conda-env/bin/python', '-m', 'pip', 'install', '-U', '-r', '/var/azureml-app/condaenv.cem6io97.requirements.txt', '--exists-action=b']\n",
      "Pip subprocess output:\n",
      "Collecting azureml-defaults==1.53.0\n",
      "  Downloading azureml_defaults-1.53.0-py3-none-any.whl (2.0 kB)\n",
      "Collecting inference-schema[numpy-support]==1.5.1\n",
      "  Downloading inference_schema-1.5.1-py3-none-any.whl (21 kB)\n",
      "Collecting joblib==1.2.0\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.0/298.0 kB 11.2 MB/s eta 0:00:00\n",
      "Collecting azureml-inference-server-http~=0.8.0\n",
      "  Downloading azureml_inference_server_http-0.8.4.2-py3-none-any.whl (43 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.5/43.5 kB 10.4 MB/s eta 0:00:00\n",
      "Collecting azureml-dataset-runtime[fuse]~=1.53.0\n",
      "  Downloading azureml_dataset_runtime-1.53.0-py3-none-any.whl (2.3 kB)\n",
      "Collecting azureml-core~=1.53.0\n",
      "  Downloading azureml_core-1.53.0-py3-none-any.whl (3.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 27.6 MB/s eta 0:00:00\n",
      "Collecting wrapt<=1.12.1,>=1.11.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting python-dateutil>=2.5.3\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.9/229.9 kB 32.4 MB/s eta 0:00:00\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 505.5/505.5 kB 30.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.13.0 in /opt/miniconda/envs/inf-conda-env/lib/python3.9/site-packages (from inference-schema[numpy-support]==1.5.1->-r /var/azureml-app/condaenv.cem6io97.requirements.txt (line 2)) (1.23.5)\n",
      "Collecting humanfriendly<11.0,>=4.7\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 14.3 MB/s eta 0:00:00\n",
      "Collecting azure-mgmt-resource<=22.0.0,>=15.0.0\n",
      "  Downloading azure_mgmt_resource-22.0.0-py3-none-any.whl (2.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 32.3 MB/s eta 0:00:00\n",
      "Collecting jmespath<2.0.0\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting packaging<=23.0,>=20.0\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.7/42.7 kB 9.3 MB/s eta 0:00:00\n",
      "Collecting PyJWT<3.0.0\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Collecting msrest<=0.7.1,>=0.5.1\n",
      "  Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.4/85.4 kB 25.0 MB/s eta 0:00:00\n",
      "Collecting azure-mgmt-authorization<4,>=0.40.0\n",
      "  Downloading azure_mgmt_authorization-3.0.0-py3-none-any.whl (965 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 965.9/965.9 kB 31.5 MB/s eta 0:00:00\n",
      "Collecting ndg-httpsclient<=0.5.1\n",
      "  Downloading ndg_httpsclient-0.5.1-py3-none-any.whl (34 kB)\n",
      "Collecting pyopenssl<24.0.0\n",
      "  Downloading pyOpenSSL-23.3.0-py3-none-any.whl (58 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 16.4 MB/s eta 0:00:00\n",
      "Collecting msal<2.0.0,>=1.15.0\n",
      "  Downloading msal-1.28.0-py3-none-any.whl (102 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.2/102.2 kB 16.5 MB/s eta 0:00:00\n",
      "Collecting jsonpickle<4.0.0\n",
      "  Downloading jsonpickle-3.0.4-py3-none-any.whl (39 kB)\n",
      "Collecting docker<7.0.0\n",
      "  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148.1/148.1 kB 24.3 MB/s eta 0:00:00\n",
      "Collecting cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*\n",
      "  Downloading cryptography-42.0.7-cp39-abi3-manylinux_2_28_x86_64.whl (3.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 30.4 MB/s eta 0:00:00\n",
      "Collecting knack~=0.10.0\n",
      "  Downloading knack-0.10.1-py3-none-any.whl (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.1/61.1 kB 20.6 MB/s eta 0:00:00\n",
      "Collecting pathspec<1.0.0\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Collecting paramiko<4.0.0,>=2.0.8\n",
      "  Downloading paramiko-3.4.0-py3-none-any.whl (225 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 225.9/225.9 kB 41.9 MB/s eta 0:00:00\n",
      "Collecting pkginfo\n",
      "  Downloading pkginfo-1.10.0-py3-none-any.whl (30 kB)\n",
      "Collecting backports.tempfile\n",
      "  Downloading backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting SecretStorage<4.0.0\n",
      "  Downloading SecretStorage-3.3.3-py3-none-any.whl (15 kB)\n",
      "Collecting msal-extensions<=1.0.0,>=0.3.0\n",
      "  Downloading msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting azure-mgmt-keyvault<11.0.0,>=0.40.0\n",
      "  Downloading azure_mgmt_keyvault-10.3.0-py3-none-any.whl (933 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 933.0/933.0 kB 31.7 MB/s eta 0:00:00\n",
      "Collecting adal<=1.2.7,>=1.2.0\n",
      "  Downloading adal-1.2.7-py2.py3-none-any.whl (55 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.5/55.5 kB 17.1 MB/s eta 0:00:00\n",
      "Collecting contextlib2<22.0.0\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting urllib3<2.0.0,>=1.23\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.8/143.8 kB 24.8 MB/s eta 0:00:00\n",
      "Collecting argcomplete<3\n",
      "  Downloading argcomplete-2.1.2-py3-none-any.whl (37 kB)\n",
      "Collecting azure-mgmt-storage<=21.0.0,>=16.0.0\n",
      "  Downloading azure_mgmt_storage-21.0.0-py3-none-any.whl (2.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 32.1 MB/s eta 0:00:00\n",
      "Collecting msrestazure<=0.6.4,>=0.4.33\n",
      "  Downloading msrestazure-0.6.4-py2.py3-none-any.whl (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.5/40.5 kB 21.6 MB/s eta 0:00:00\n",
      "Collecting azure-mgmt-network==21.0.1\n",
      "  Downloading azure_mgmt_network-21.0.1-py3-none-any.whl (8.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.9/8.9 MB 28.1 MB/s eta 0:00:00\n",
      "Collecting azure-graphrbac<1.0.0,>=0.40.0\n",
      "  Downloading azure_graphrbac-0.61.1-py2.py3-none-any.whl (141 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.4/141.4 kB 20.3 MB/s eta 0:00:00\n",
      "Collecting azure-core<2.0.0\n",
      "  Downloading azure_core-1.30.1-py3-none-any.whl (193 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.4/193.4 kB 30.3 MB/s eta 0:00:00\n",
      "Collecting azure-common<2.0.0,>=1.1.12\n",
      "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: requests[socks]<3.0.0,>=2.19.1 in /opt/miniconda/envs/inf-conda-env/lib/python3.9/site-packages (from azureml-core~=1.53.0->azureml-defaults==1.53.0->-r /var/azureml-app/condaenv.cem6io97.requirements.txt (line 1)) (2.32.2)\n",
      "Collecting azure-mgmt-containerregistry<11,>=8.2.0\n",
      "  Downloading azure_mgmt_containerregistry-10.3.0-py3-none-any.whl (2.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 31.9 MB/s eta 0:00:00\n",
      "Collecting azure-mgmt-core<2.0.0,>=1.3.1\n",
      "  Downloading azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB)\n",
      "Collecting pyarrow<=11.0.0,>=0.17.0\n",
      "  Downloading pyarrow-11.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.9/34.9 MB 20.0 MB/s eta 0:00:00\n",
      "Collecting azureml-dataprep<4.14.0a,>=4.12.0a\n",
      "  Downloading azureml_dataprep-4.12.10-py3-none-any.whl (38.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.2/38.2 MB 26.3 MB/s eta 0:00:00\n",
      "Collecting fusepy<4.0.0,>=3.0.1\n",
      "  Downloading fusepy-3.0.1.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pydantic<1.11,>=1.9\n",
      "  Downloading pydantic-1.10.15-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 29.1 MB/s eta 0:00:00\n",
      "Collecting gunicorn==20.1.0\n",
      "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 13.6 MB/s eta 0:00:00\n",
      "Collecting flask<2.3.0\n",
      "  Downloading Flask-2.2.5-py3-none-any.whl (101 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.8/101.8 kB 34.3 MB/s eta 0:00:00\n",
      "Collecting flask-cors~=3.0.1\n",
      "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
      "Collecting azureml-inference-server-http~=0.8.0\n",
      "  Downloading azureml_inference_server_http-0.8.4.1-py3-none-any.whl (43 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.5/43.5 kB 15.8 MB/s eta 0:00:00\n",
      "Collecting opencensus-ext-azure~=1.1.0\n",
      "  Downloading opencensus_ext_azure-1.1.13-py2.py3-none-any.whl (43 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.4/43.4 kB 15.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools>=3.0 in /opt/miniconda/envs/inf-conda-env/lib/python3.9/site-packages (from gunicorn==20.1.0->azureml-inference-server-http~=0.8.0->azureml-defaults==1.53.0->-r /var/azureml-app/condaenv.cem6io97.requirements.txt (line 1)) (70.0.0)\n",
      "Collecting six>=1.5\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting typing-extensions>=4.6.0\n",
      "  Downloading typing_extensions-4.12.0-py3-none-any.whl (37 kB)\n",
      "Collecting isodate<1.0.0,>=0.6.1\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.7/41.7 kB 18.7 MB/s eta 0:00:00\n",
      "Collecting azureml-dataprep-native<39.0.0,>=38.0.0\n",
      "  Downloading azureml_dataprep_native-38.0.0-cp39-cp39-manylinux1_x86_64.whl (186 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 186.2/186.2 kB 5.0 MB/s eta 0:00:00\n",
      "Collecting azure-identity>=1.7.0\n",
      "  Downloading azure_identity-1.16.0-py3-none-any.whl (166 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.1/166.1 kB 22.7 MB/s eta 0:00:00\n",
      "Collecting azureml-dataprep-rslex~=2.19.7dev0\n",
      "  Downloading azureml_dataprep_rslex-2.19.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.5/23.5 MB 28.1 MB/s eta 0:00:00\n",
      "Collecting cloudpickle<3.0.0,>=1.1.0\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting jsonschema\n",
      "  Downloading jsonschema-4.22.0-py3-none-any.whl (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 18.9 MB/s eta 0:00:00\n",
      "Collecting dotnetcore2<4.0.0,>=3.0.0\n",
      "  Downloading dotnetcore2-3.1.23-py3-none-manylinux1_x86_64.whl (31.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 31.1/31.1 MB 25.9 MB/s eta 0:00:00\n",
      "Collecting pyyaml<7.0.0,>=5.1.0\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 738.9/738.9 kB 41.4 MB/s eta 0:00:00\n",
      "Collecting cffi>=1.12\n",
      "  Downloading cffi-1.16.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 443.4/443.4 kB 36.8 MB/s eta 0:00:00\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 14.6 MB/s eta 0:00:00\n",
      "Collecting click>=8.0\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 25.6 MB/s eta 0:00:00\n",
      "Collecting importlib-metadata>=3.6.0\n",
      "  Downloading importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\n",
      "Collecting Jinja2>=3.0\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.3/133.3 kB 22.4 MB/s eta 0:00:00\n",
      "Collecting Werkzeug>=2.2.2\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.3/227.3 kB 24.2 MB/s eta 0:00:00\n",
      "Collecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting pygments\n",
      "  Downloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 28.1 MB/s eta 0:00:00\n",
      "Collecting portalocker<3,>=1.0\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda/envs/inf-conda-env/lib/python3.9/site-packages (from msrest<=0.7.1,>=0.5.1->azureml-core~=1.53.0->azureml-defaults==1.53.0->-r /var/azureml-app/condaenv.cem6io97.requirements.txt (line 1)) (2024.2.2)\n",
      "Collecting requests-oauthlib>=0.5.0\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting pyasn1>=0.1.1\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.3/85.3 kB 23.9 MB/s eta 0:00:00\n",
      "Collecting opencensus<1.0.0,>=0.11.4\n",
      "  Downloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.2/128.2 kB 23.4 MB/s eta 0:00:00\n",
      "Collecting psutil>=5.6.3\n",
      "  Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 288.2/288.2 kB 31.8 MB/s eta 0:00:00\n",
      "Collecting bcrypt>=3.2\n",
      "  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 283.7/283.7 kB 22.8 MB/s eta 0:00:00\n",
      "Collecting pynacl>=1.5\n",
      "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 856.7/856.7 kB 31.9 MB/s eta 0:00:00\n",
      "Collecting cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*\n",
      "  Downloading cryptography-41.0.7-cp37-abi3-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 31.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda/envs/inf-conda-env/lib/python3.9/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.53.0->azureml-defaults==1.53.0->-r /var/azureml-app/condaenv.cem6io97.requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda/envs/inf-conda-env/lib/python3.9/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.53.0->azureml-defaults==1.53.0->-r /var/azureml-app/condaenv.cem6io97.requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/miniconda/envs/inf-conda-env/lib/python3.9/site-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core~=1.53.0->azureml-defaults==1.53.0->-r /var/azureml-app/condaenv.cem6io97.requirements.txt (line 1)) (1.7.1)\n",
      "Collecting jeepney>=0.6\n",
      "  Downloading jeepney-0.8.0-py3-none-any.whl (48 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.4/48.4 kB 24.3 MB/s eta 0:00:00\n",
      "Collecting backports.weakref\n",
      "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl (5.2 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.6/117.6 kB 24.3 MB/s eta 0:00:00\n",
      "Collecting distro>=1.2.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.18.2-py3-none-any.whl (8.3 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting google-api-core<3.0.0,>=1.0.0\n",
      "  Downloading google_api_core-2.19.0-py3-none-any.whl (139 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.0/139.0 kB 36.2 MB/s eta 0:00:00\n",
      "Collecting opencensus-context>=0.1.3\n",
      "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 24.7 MB/s eta 0:00:00\n",
      "Collecting referencing>=0.28.4\n",
      "  Downloading referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Collecting attrs>=22.2.0\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.8/60.8 kB 11.2 MB/s eta 0:00:00\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.18.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 32.0 MB/s eta 0:00:00\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.1/229.1 kB 30.7 MB/s eta 0:00:00\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.6/294.6 kB 27.7 MB/s eta 0:00:00\n",
      "Collecting google-auth<3.0.dev0,>=2.14.1\n",
      "  Downloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 189.2/189.2 kB 29.4 MB/s eta 0:00:00\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3\n",
      "  Downloading proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.8/48.8 kB 26.3 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.2/181.2 kB 33.2 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Building wheels for collected packages: wrapt, fusepy\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp39-cp39-linux_x86_64.whl size=36967 sha256=ae5f5471f0baf688840f86784f909328f292779e4c6e2f3bd99ea4f79a7ab6d7\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/23/68/efe259aaca055e93b08e74fbe512819c69a2155c11ba3c0f10\n",
      "  Building wheel for fusepy (setup.py): started\n",
      "  Building wheel for fusepy (setup.py): finished with status 'done'\n",
      "  Created wheel for fusepy: filename=fusepy-3.0.1-py3-none-any.whl size=10487 sha256=e26c902c46442397a1fd1c326f07186e061acd7917bbc8edc5136c7c9156f904\n",
      "  Stored in directory: /root/.cache/pip/wheels/d5/60/5d/807081f971b004ab5c86eba746c6b5e72d0258215bf2425d68\n",
      "Successfully built wrapt fusepy\n",
      "Installing collected packages: wrapt, pytz, opencensus-context, fusepy, backports.weakref, azureml-dataprep-rslex, azureml-dataprep-native, azure-common, zipp, websocket-client, urllib3, typing-extensions, tabulate, six, rpds-py, pyyaml, PyJWT, pygments, pycparser, pyasn1, pyarrow, psutil, protobuf, portalocker, pkginfo, pathspec, packaging, oauthlib, MarkupSafe, jsonpickle, joblib, jmespath, jeepney, itsdangerous, humanfriendly, gunicorn, distro, contextlib2, cloudpickle, click, cachetools, bcrypt, backports.tempfile, attrs, argcomplete, Werkzeug, rsa, referencing, python-dateutil, pydantic, pyasn1-modules, proto-plus, knack, Jinja2, isodate, importlib-metadata, googleapis-common-protos, dotnetcore2, cffi, requests-oauthlib, pynacl, jsonschema-specifications, inference-schema, google-auth, flask, docker, cryptography, azure-core, SecretStorage, pyopenssl, paramiko, msrest, jsonschema, google-api-core, flask-cors, azure-mgmt-core, adal, opencensus, ndg-httpsclient, msrestazure, msal, azure-mgmt-storage, azure-mgmt-resource, azure-mgmt-network, azure-mgmt-keyvault, azure-mgmt-containerregistry, azure-mgmt-authorization, msal-extensions, azure-graphrbac, azureml-core, azure-identity, opencensus-ext-azure, azureml-dataprep, azureml-inference-server-http, azureml-dataset-runtime, azureml-defaults\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.4.2\n",
      "    Uninstalling joblib-1.4.2:\n",
      "      Successfully uninstalled joblib-1.4.2\n",
      "Successfully installed Jinja2-3.1.4 MarkupSafe-2.1.5 PyJWT-2.8.0 SecretStorage-3.3.3 Werkzeug-3.0.3 adal-1.2.7 argcomplete-2.1.2 attrs-23.2.0 azure-common-1.1.28 azure-core-1.30.1 azure-graphrbac-0.61.1 azure-identity-1.16.0 azure-mgmt-authorization-3.0.0 azure-mgmt-containerregistry-10.3.0 azure-mgmt-core-1.4.0 azure-mgmt-keyvault-10.3.0 azure-mgmt-network-21.0.1 azure-mgmt-resource-22.0.0 azure-mgmt-storage-21.0.0 azureml-core-1.53.0 azureml-dataprep-4.12.10 azureml-dataprep-native-38.0.0 azureml-dataprep-rslex-2.19.8 azureml-dataset-runtime-1.53.0 azureml-defaults-1.53.0 azureml-inference-server-http-0.8.4.1 backports.tempfile-1.0 backports.weakref-1.0.post1 bcrypt-4.1.3 cachetools-5.3.3 cffi-1.16.0 click-8.1.7 cloudpickle-2.2.1 contextlib2-21.6.0 cryptography-41.0.7 distro-1.9.0 docker-6.1.3 dotnetcore2-3.1.23 flask-2.2.5 flask-cors-3.0.10 fusepy-3.0.1 google-api-core-2.19.0 google-auth-2.29.0 googleapis-common-protos-1.63.0 gunicorn-20.1.0 humanfriendly-10.0 importlib-metadata-7.1.0 inference-schema-1.5.1 isodate-0.6.1 itsdangerous-2.2.0 jeepney-0.8.0 jmespath-1.0.1 joblib-1.2.0 jsonpickle-3.0.4 jsonschema-4.22.0 jsonschema-specifications-2023.12.1 knack-0.10.1 msal-1.28.0 msal-extensions-1.0.0 msrest-0.7.1 msrestazure-0.6.4 ndg-httpsclient-0.5.1 oauthlib-3.2.2 opencensus-0.11.4 opencensus-context-0.1.3 opencensus-ext-azure-1.1.13 packaging-23.0 paramiko-3.4.0 pathspec-0.12.1 pkginfo-1.10.0 portalocker-2.8.2 proto-plus-1.23.0 protobuf-4.25.3 psutil-5.9.8 pyarrow-11.0.0 pyasn1-0.6.0 pyasn1-modules-0.4.0 pycparser-2.22 pydantic-1.10.15 pygments-2.18.0 pynacl-1.5.0 pyopenssl-23.3.0 python-dateutil-2.9.0.post0 pytz-2024.1 pyyaml-6.0.1 referencing-0.35.1 requests-oauthlib-2.0.0 rpds-py-0.18.1 rsa-4.9 six-1.16.0 tabulate-0.9.0 typing-extensions-4.12.0 urllib3-1.26.18 websocket-client-1.8.0 wrapt-1.12.1 zipp-3.18.2\n",
      "\n",
      "done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate inf-conda-env\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      ".... ---> 957ab27ca0f2\n",
      "Step 6/6 : CMD [\"conda\", \"run\", \"--no-capture-output\", \"-n\", \"inf-conda-env\", \"runsvdir\", \"/var/runit\"]\n",
      " ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n",
      " ---> Running in 9c151f06d8d8\n",
      " ---> f37b2071e31c\n",
      "Successfully built f37b2071e31c\n",
      "Successfully tagged endpt-05240930390654:blue\n",
      "\n",
      "Starting up endpoint...Done (2m 10s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ManagedOnlineDeployment({'private_network_connection': None, 'package_model': False, 'provisioning_state': 'Succeeded', 'endpoint_name': 'endpt-05240930390654', 'type': 'Managed', 'name': 'blue', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': PosixPath('/Users/ben.dixon/Repos/azure-experiments/notebooks'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x118f2d4f0>, 'model': Model({'job_name': None, 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': '7713d7a5680d37a33a7ac52530aec294', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': PosixPath('/Users/ben.dixon/Repos/azure-experiments/notebooks'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x118f3f0d0>, 'version': '1', 'latest_version': None, 'path': '/Users/ben.dixon/Repos/azure-experiments/notebooks/model-1/model/sklearn_regression_model.pkl', 'datastore': None, 'utc_time_created': None, 'flavors': None, 'arm_type': 'model_version', 'type': 'custom_model', 'stage': None}), 'code_configuration': {'code': 'model-1/onlinescoring'}, 'environment': Environment({'arm_type': 'environment_version', 'latest_version': None, 'image': 'mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest', 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'CliV2AnonymousEnvironment', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': PosixPath('/Users/ben.dixon/Repos/azure-experiments/notebooks'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x118f5e250>, 'version': '8a21e84929f87690a130d2728db9597c', 'conda_file': {'name': 'model-env', 'channels': ['conda-forge'], 'dependencies': ['python=3.9', 'numpy=1.23.5', 'pip=23.0.1', 'scikit-learn=1.2.2', 'scipy=1.10.1', {'pip': ['azureml-defaults==1.53.0', 'inference-schema[numpy-support]==1.5.1', 'joblib==1.2.0']}]}, 'build': None, 'inference_config': None, 'os_type': None, 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': 'channels:\\n- conda-forge\\ndependencies:\\n- python=3.9\\n- numpy=1.23.5\\n- pip=23.0.1\\n- scikit-learn=1.2.2\\n- scipy=1.10.1\\n- pip:\\n  - azureml-defaults==1.53.0\\n  - inference-schema[numpy-support]==1.5.1\\n  - joblib==1.2.0\\nname: model-env\\n'}), 'environment_variables': {}, 'app_insights_enabled': False, 'scale_settings': None, 'request_settings': None, 'liveness_probe': None, 'readiness_probe': None, 'instance_count': 1, 'arm_type': 'online_deployment', 'model_mount_path': None, 'instance_type': 'local', 'data_collector': None, 'egress_public_network_access': None})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_deployments.begin_create_or_update(\n",
    "    deployment=blue_deployment, local=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `local=True` flag directs the SDK to deploy the endpoint in the Docker environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Verify the local deployment succeeded\n",
    "\n",
    "## 4.1 Check the status to see whether the model was deployed without error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ManagedOnlineEndpoint({'public_network_access': None, 'provisioning_state': 'Succeeded', 'scoring_uri': 'http://localhost:55002/score', 'openapi_uri': None, 'name': 'endpt-05240930390654', 'description': 'this is a sample online endpoint', 'tags': {'foo': 'bar'}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': PosixPath('/Users/ben.dixon/Repos/azure-experiments/notebooks'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x118f70100>, 'auth_mode': 'key', 'location': 'local', 'identity': None, 'traffic': {}, 'mirror_traffic': {}, 'kind': None})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_endpoints.get(name=endpoint_name, local=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Get logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"2024-05-24T08:38:36,362697764+00:00 | gunicorn/run | \\r\\n2024-05-24T08:38:37,327502583+00:00 | gunicorn/run | \\r\\n2024-05-24T08:38:37,345203667+00:00 | gunicorn/run | ###############################################\\r\\n2024-05-24T08:38:37,361655417+00:00 | gunicorn/run | AzureML Inference Server\\r\\n2024-05-24T08:38:37,383157333+00:00 | gunicorn/run | ###############################################\\r\\n2024-05-24T08:38:37,403171875+00:00 | gunicorn/run | \\r\\n2024-05-24T08:38:37,421930875+00:00 | gunicorn/run | Starting AzureML Inference Server HTTP.\\r\\n2024-05-24 08:38:37,752 I [21] azmlinfsrv - Loaded logging config from /opt/miniconda/envs/inf-conda-env/lib/python3.9/site-packages/azureml_inference_server_http/logging.json\\r\\n\\r\\nAzure ML Inferencing HTTP server v0.8.4.1\\r\\n\\r\\n\\r\\nServer Settings\\r\\n---------------\\r\\nEntry Script Name: /var/azureml-app/onlinescoring/score.py\\r\\nModel Directory: /var/azureml-app/azureml-models//7713d7a5680d37a33a7ac52530aec294/1\\r\\nConfig File: None\\r\\nWorker Count: 1\\r\\nWorker Timeout (seconds): 300\\r\\nServer Port: 31311\\r\\nHealth Port: 31311\\r\\nApplication Insights Enabled: false\\r\\nApplication Insights Key: None\\r\\nInferencing HTTP server version: azmlinfsrv/0.8.4.1\\r\\nCORS for the specified origins: None\\r\\nCreate dedicated endpoint for health: None\\r\\n\\r\\n\\r\\nServer Routes\\r\\n---------------\\r\\nLiveness Probe: GET   127.0.0.1:31311/\\r\\nScore:          POST  127.0.0.1:31311/score\\r\\n\\r\\n2024-05-24 08:38:37,864 I [21] gunicorn.error - Starting gunicorn 20.1.0\\r\\n2024-05-24 08:38:37,866 I [21] gunicorn.error - Listening at: http://0.0.0.0:31311 (21)\\r\\n2024-05-24 08:38:37,866 I [21] gunicorn.error - Using worker: sync\\r\\n2024-05-24 08:38:37,870 I [86] gunicorn.error - Booting worker with pid: 86\\r\\n2024-05-24 08:38:38,513 I [86] azmlinfsrv - AML_FLASK_ONE_COMPATIBILITY is set. Patched Flask to ensure compatibility with Flask 1.\\r\\nInitializing logger\\r\\n2024-05-24 08:38:38,517 I [86] azmlinfsrv - Starting up app insights client\\r\\n2024-05-24 08:38:38,904 I [86] azmlinfsrv.user_script - Found user script at /var/azureml-app/onlinescoring/score.py\\r\\n2024-05-24 08:38:38,904 I [86] azmlinfsrv.user_script - run() is not decorated. Server will invoke it with the input in JSON string.\\r\\n2024-05-24 08:38:38,904 I [86] azmlinfsrv.user_script - Invoking user's init function\\r\\n/opt/miniconda/envs/inf-conda-env/lib/python3.9/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearRegression from version 0.24.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\\r\\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\\r\\n  warnings.warn(\\r\\n2024-05-24 08:38:39,588 I [86] azmlinfsrv.user_script - Users's init has completed successfully\\r\\n2024-05-24 08:38:39,591 I [86] azmlinfsrv.swagger - Swaggers are prepared for the following versions: [2, 3, 3.1].\\r\\n2024-05-24 08:38:39,591 I [86] azmlinfsrv - Scoring timeout is set to 3600000\\r\\n2024-05-24 08:38:39,591 I [86] azmlinfsrv - Worker with pid 86 ready for serving traffic\\r\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_deployments.get_logs(\n",
    "    name=\"blue\", endpoint_name=endpoint_name, local=True, lines=50\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Invoke the local endpoint\n",
    "Invoke the endpoint to score the model by using the convenience command invoke and passing query parameters that are stored in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=endpoint_name,\n",
    "    request_file=\"model-1/sample-request.json\",\n",
    "    local=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing xgboost endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1, 0, 2, 1]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=\"xgboost-model-endpoint-13cb9b54\",\n",
    "    request_file=\"model-1/xgboost-request.json\",\n",
    "    local=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Deploy your online endpoint to Azure\n",
    "Next, deploy your online endpoint to Azure.\n",
    "\n",
    "## 5.1 Create the endpoint\n",
    "Using the `endpoint` we defined earlier and the `MLClient` created earlier, we'll now create the endpoint in the workspace. This command will start the endpoint creation and return a confirmation response while the endpoint creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://endpt-05240930390654.ukwest.inference.ml.azure.com/score', 'openapi_uri': 'https://endpt-05240930390654.ukwest.inference.ml.azure.com/swagger.json', 'name': 'endpt-05240930390654', 'description': 'this is a sample online endpoint', 'tags': {'foo': 'bar'}, 'properties': {'createdBy': 'Ben Dixon', 'createdAt': '2024-05-24T08:40:12.837223+0000', 'lastModifiedAt': '2024-05-24T08:40:12.837223+0000', 'azureml.onlineendpointid': '/subscriptions/3979595d-b401-4ff4-b3ef-661b8481c742/resourcegroups/aiplatform-1/providers/microsoft.machinelearningservices/workspaces/ai-mlw-dev-01/onlineendpoints/endpt-05240930390654', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/3979595d-b401-4ff4-b3ef-661b8481c742/providers/Microsoft.MachineLearningServices/locations/ukwest/mfeOperationsStatus/oe:3a3fd522-5898-4524-9e4b-7f1074ac8ae4:e12410ad-b98c-415d-a403-b15663069a38?api-version=2022-02-01-preview'}, 'print_as_yaml': False, 'id': '/subscriptions/3979595d-b401-4ff4-b3ef-661b8481c742/resourceGroups/aiplatform-1/providers/Microsoft.MachineLearningServices/workspaces/ai-mlw-dev-01/onlineEndpoints/endpt-05240930390654', 'Resource__source_path': '', 'base_path': '/Users/ben.dixon/Repos/azure-experiments/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x119509eb0>, 'auth_mode': 'key', 'location': 'ukwest', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x119503220>, 'traffic': {}, 'mirror_traffic': {}, 'kind': 'Managed'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Create the deployment\n",
    "\n",
    "Using the `blue_deployment` that we defined earlier and the `MLClient` we created earlier, we'll now create the deployment in the workspace. This command will start the deployment creation and return a confirmation response while the deployment creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.begin_create_or_update(blue_deployment).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blue deployment takes 100 traffic\n",
    "endpoint.traffic = {\"blue\": 100}\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Test the endpoint with sample data\n",
    "Using the `MLClient` created earlier, we will get a handle to the endpoint. The endpoint can be invoked using the `invoke` command with the following parameters:\n",
    "- `endpoint_name` - Name of the endpoint\n",
    "- `request_file` - File with request data\n",
    "- `deployment_name` - Name of the specific deployment to test in an endpoint\n",
    "\n",
    "We will send a sample request using a [json](./model-1/sample-request.json) file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the blue deployment with some sample data\n",
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=endpoint_name,\n",
    "    deployment_name=\"blue\",\n",
    "    request_file=\"../model-1/sample-request.json\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Managing endpoints and deployments\n",
    "\n",
    "## 7.1 Get details of the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the details for online endpoint\n",
    "endpoint = ml_client.online_endpoints.get(name=endpoint_name)\n",
    "\n",
    "# existing traffic details\n",
    "print(endpoint.traffic)\n",
    "\n",
    "# Get the scoring URI\n",
    "print(endpoint.scoring_uri)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Get the logs for the new deployment\n",
    "Get the logs for the green deployment and verify as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.get_logs(\n",
    "    name=\"blue\", endpoint_name=endpoint_name, lines=50\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Delete the endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "description": {
   "description": "Use an online endpoint to deploy your model, so you don't have to create and manage the underlying infrastructure"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
